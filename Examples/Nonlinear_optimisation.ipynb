{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce9d414-4416-40fe-8036-abf77f96c2ea",
   "metadata": {},
   "source": [
    "# Nonlinear optimisation\n",
    "\n",
    "This notebooks contains a number of topics for nonlinear optimisation:\n",
    "\n",
    "1. Plotting of Gradient and calculation of Hessian\n",
    "2. Analytical steepest descent method with SymPy\n",
    "3. Use of SciPy to solve nonlinear optimisation problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8b69c-0695-49d1-8bf7-9971690f600c",
   "metadata": {},
   "source": [
    "## Gradient and Hessian\n",
    "\n",
    "Let's start with plotting the gradient of a 2D function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd0a8e-f4f4-49e2-98c2-64db30fcf2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
    "\n",
    "# Define the function and the gradient\n",
    "def f(x, y):\n",
    "    return -(np.cos(x)**2 + np.cos(y)**2)**2\n",
    "\n",
    "def grad_f(x, y):\n",
    "    return 4 * (np.cos(x)**2 + np.cos(y)**2) * np.sin(x), 4 * (np.cos(x)**2 + np.cos(y)**2) * np.sin(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e5f72-a756-429f-a323-85f6ad2cc210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of x and y values\n",
    "x = np.linspace(-np.pi/2, np.pi/2, 100)\n",
    "y = np.linspace(-np.pi/2, np.pi/2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Evaluate the function on the grid\n",
    "Z = f(X, Y)\n",
    "\n",
    "# Create the figure for plotting\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the surface\n",
    "ax.plot_surface(X, Y, Z, alpha=0.5, rstride=4, cstride=4, color='b')\n",
    "\n",
    "# Plot the gradient vectors (quivers)\n",
    "# Subsample for quiver to reduce the density of the vectors\n",
    "skip = (slice(None, None, 6), slice(None, None, 5))  # Change 5 to greater numbers to reduce more\n",
    "U, V = grad_f(X[skip], Y[skip])\n",
    "# Normalization\n",
    "norm = np.sqrt(U**2 + V**2)\n",
    "U /= norm\n",
    "V /= norm\n",
    "ax.quiver(X[skip], Y[skip], -4, U, V, 0, color='r', length=0.1, normalize=True)\n",
    "\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "ax.set_title('Surface plot with gradient projection in XY-plane')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Save the figure to file\n",
    "#fig.savefig('Function_and_gradient.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ffca3-0c6b-45d7-ac03-90e490529724",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Steepest descent method\n",
    "\n",
    "We will set up an analytical solution with SymPy. Here is a link to an introduction to it: [Introduction tutorial](https://docs.sympy.org/latest/tutorials/intro-tutorial/index.html)\n",
    "\n",
    "The implementation should work for any function of three variables for which SymPy manages to calculate the gradient.\n",
    "\n",
    "First we define the function and calculate its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c772f85a-c558-4aa2-8b51-24c333a3f9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function f(x1, x2, x3) is defined as:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x_{1}^{2} + x_{1} \\left(1 - x_{2}\\right) + x_{2}^{2} - x_{2} x_{3} + x_{3}^{2} + x_{3}$"
      ],
      "text/plain": [
       "x1**2 + x1*(1 - x2) + x2**2 - x2*x3 + x3**2 + x3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient of f(x1, x2, x3) is:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}2 x_{1} - x_{2} + 1\\\\- x_{1} + 2 x_{2} - x_{3}\\\\- x_{2} + 2 x_{3} + 1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[  2*x1 - x2 + 1],\n",
       "[-x1 + 2*x2 - x3],\n",
       "[ -x2 + 2*x3 + 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Declare variables\n",
    "x1, x2, x3 = sp.symbols('x1 x2 x3')\n",
    "a = sp.symbols('a')\n",
    "\n",
    "# Define the function\n",
    "f = x1**2 + x1 * (1-x2) + x2**2 - x2*x3 + x3**2 + x3\n",
    "\n",
    "# Display the function\n",
    "print(\"The function f(x1, x2, x3) is defined as:\")\n",
    "display(f)\n",
    "\n",
    "# Compute the gradient of f\n",
    "gradient_f = sp.Matrix([f.diff(x) for x in (x1, x2, x3)])\n",
    "\n",
    "# Display the gradient\n",
    "print(\"The gradient of f(x1, x2, x3) is:\")\n",
    "display(gradient_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159c35f-3a71-478b-a61b-d80f7617d3a5",
   "metadata": {},
   "source": [
    "Now we set up a loop to iteratively calculate the next point. We start by defining the initial point. In the loop, we are performing the following steps:\n",
    "\n",
    "- Evaluate the negative gradient at the current point\n",
    "- Calculate the step and add it to the current point\n",
    "- Insert the formula for the new point into the function\n",
    "- Calculate the derivative of the function with respect to a\n",
    "- Find the optimal value of a and update x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "852d93b0-c0d4-4a8b-843f-1020c30982b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "x[1] = [ -1/2 0 -1/2 ]\n",
      "Step 2\n",
      "x[2] = [ -1/2 -1/2 -1/2 ]\n",
      "Step 3\n",
      "x[3] = [ -3/4 -1/2 -3/4 ]\n",
      "Step 4\n",
      "x[4] = [ -3/4 -3/4 -3/4 ]\n",
      "Step 5\n",
      "x[5] = [ -7/8 -3/4 -7/8 ]\n",
      "Step 6\n",
      "x[6] = [ -7/8 -7/8 -7/8 ]\n",
      "Step 7\n",
      "x[7] = [ -15/16 -7/8 -15/16 ]\n",
      "Step 8\n",
      "x[8] = [ -15/16 -15/16 -15/16 ]\n"
     ]
    }
   ],
   "source": [
    "# Set the initial point\n",
    "x = [0, 0, 0]\n",
    "\n",
    "# Set to True to show debugging output\n",
    "show = False\n",
    "\n",
    "for i in range(8):\n",
    "    print(\"Step\", i+1)\n",
    "    # Evaluate the negative gradient at the current point\n",
    "    grad_at_x = -gradient_f.subs({x1: x[0], x2: x[1], x3: x[2]}) \n",
    "    if show:\n",
    "        display(grad_at_x)\n",
    "\n",
    "    # Calculate the step and add it to the current point\n",
    "    x_update = [a * g for g in grad_at_x]\n",
    "    x_new = [sum(i) for i in zip(x, x_update)]\n",
    "\n",
    "    if show:\n",
    "        display(x_update)\n",
    "        display(x_new)\n",
    "    \n",
    "    # Insert the formula for the new point into the function\n",
    "    f_new = f.subs({x1: x_new[0], x2: x_new[1], x3: x_new[2]}) \n",
    "\n",
    "    # Calculate the derivative of the function with respect to a\n",
    "    df_da = sp.diff(f_new, a)\n",
    "\n",
    "    if show:\n",
    "        display(f_new)\n",
    "        display(df_da)\n",
    "\n",
    "    # Find the optimal value of a\n",
    "    sol = sp.solvers.solve(df_da, a)\n",
    "\n",
    "    # Update x\n",
    "    x_step = [sol[0] * g for g in grad_at_x]\n",
    "\n",
    "    if show:\n",
    "        display(sol)\n",
    "        display(x_step)\n",
    "\n",
    "    # Add the step to the current x\n",
    "    x = [sum(i) for i in zip(x, x_step)]\n",
    "    print(\"x[{}] = [\".format(i+1), *x, \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff3e29-bfa9-4daf-a39a-2d48fc5b99ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67903955-b3a6-4b14-90a0-1dc28c9aded3",
   "metadata": {},
   "source": [
    "## Use SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "852b6992-457b-41dd-90c4-f190c5383681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       message: Optimization terminated successfully.\n",
      "       success: True\n",
      "        status: 0\n",
      "           fun: -1077.9999999999989\n",
      "             x: [ 7.000e+00  7.000e+00]\n",
      "           nit: 95\n",
      "          nfev: 183\n",
      " final_simplex: (array([[ 7.000e+00,  7.000e+00],\n",
      "                       [ 7.000e+00,  7.000e+00],\n",
      "                       [ 7.000e+00,  7.000e+00]]), array([-1.078e+03, -1.078e+03, -1.078e+03]))\n",
      "Function value 1077.9999999999989\n"
     ]
    }
   ],
   "source": [
    "def problem_b(x):\n",
    "    \"\"\"Problem formulation\"\"\"\n",
    "    return -(126*x[0] - 9*x[0]**2 + 182*x[1] - 13*x[1]**2)\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "x0 = [0, 0]\n",
    "\n",
    "res = minimize(problem_b, x0, method='Nelder-Mead', tol=1e-6)\n",
    "print(res)\n",
    "print(\"Function value {}\".format(-problem_b(res.x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf577b7-6f04-4dcd-9894-e58c702780cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
